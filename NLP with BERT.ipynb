{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP with BERT.ipynb","provenance":[],"authorship_tag":"ABX9TyNZModIKSDU5KF/Qd4Ake9Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b4i0Cu0vurw0","executionInfo":{"status":"ok","timestamp":1636765397853,"user_tz":-540,"elapsed":2916,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}},"outputId":"1e3ec765-b692-4949-b098-d0bec575dfa4"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y7YQXN1MxZtl","executionInfo":{"status":"ok","timestamp":1636765397854,"user_tz":-540,"elapsed":8,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}},"outputId":"d339ea75-7b85-4636-af58-9d0fc623fcfb"},"source":["%cd drive/MyDrive/Colab Notebooks/disaster_tweets"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/disaster_tweets\n"]}]},{"cell_type":"markdown","metadata":{"id":"8EvizFu3u1ph"},"source":["## <b> 0. Bert Hyperparameters </b>"]},{"cell_type":"code","metadata":{"id":"6qN96RJRu36S","executionInfo":{"status":"ok","timestamp":1636765397854,"user_tz":-540,"elapsed":5,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"source":["random_state_split = 42\n","Dropout_num = 0\n","learning_rate = 5.95e-6\n","valid = 0.15\n","epochs_num = 3\n","batch_size_num = 16\n","target_corrected = False\n","target_big_corrected = False"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qC6A5uGru6Yy"},"source":["## <b> 1. Import Basic Libraries </b>"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HjLM8yl-u5x_","executionInfo":{"status":"ok","timestamp":1636765410228,"user_tz":-540,"elapsed":12378,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}},"outputId":"2ea18824-7a57-409f-c0a6-74ea2019376c"},"source":["import pandas as pd\n","import numpy as np\n","import os\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as mpatches\n","import seaborn as sns\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import punkt\n","from nltk.tokenize import word_tokenize\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.decomposition import PCA, TruncatedSVD\n","from sklearn.metrics import classification_report,confusion_matrix\n","\n","from collections import defaultdict\n","from collections import Counter\n","plt.style.use('ggplot')\n","\n","import string\n","import re\n","import gensim\n","\n","from wordcloud import WordCloud\n","\n","from tqdm import tqdm\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\n","from keras.initializers import Constant\n","from tensorflow.keras.optimizers import Adam # - Works\n","\n","\n","import torch\n","\n","import warnings\n","warnings.simplefilter('ignore')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","metadata":{"id":"yFWKP1EA0y34","executionInfo":{"status":"ok","timestamp":1636765612246,"user_tz":-540,"elapsed":510,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import tensorflow_hub as hub\n","\n","from bert import bert_tokenization"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mePTOlZjvGD3"},"source":["## <b> 2. Download Data </b>"]},{"cell_type":"code","metadata":{"id":"Lr5F4NEwvFkA","executionInfo":{"status":"ok","timestamp":1636765410228,"user_tz":-540,"elapsed":12,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"source":["train= pd.read_csv('./data/train.csv')\n","test=pd.read_csv('./data/test.csv')\n","submission = pd.read_csv(\"./data/sample_submission.csv\")"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"56mWYphFw4am","executionInfo":{"status":"ok","timestamp":1636765518243,"user_tz":-540,"elapsed":35073,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"source":["# Load BERT from the Tensorflow Hub\n","module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\" # 24 layer, 1024 hidden dimension(단어 하나의 임베딩 차원), Attention Head 16개\n","bert_layer = hub.KerasLayer(module_url, trainable=True)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QN6Z23i2vuVc"},"source":["## <b> 3. BERT using TFHub </b>"]},{"cell_type":"code","metadata":{"id":"6IHtLHc8vsme","executionInfo":{"status":"ok","timestamp":1636765518789,"user_tz":-540,"elapsed":555,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"source":["# We will use the official tokenization script created by the Google team\n","!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-AcqEH-iwMJD","executionInfo":{"status":"ok","timestamp":1636765521811,"user_tz":-540,"elapsed":3024,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}},"outputId":"87b272ca-f78f-489f-cf3d-36e8b1dada97"},"source":["!pip install bert-for-tf2"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.7/dist-packages (0.14.9)\n","Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.10.2)\n","Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.8.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.62.3)\n"]}]},{"cell_type":"code","metadata":{"id":"542Gy5sHv2T6","executionInfo":{"status":"ok","timestamp":1636765883423,"user_tz":-540,"elapsed":394,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"source":["# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n","def bert_encode(texts, tokenizer, max_len=512):\n","    all_tokens = []\n","    all_masks = []\n","    all_segments = []\n","    \n","    for text in texts:\n","        text = tokenizer.tokenize(text)\n","            \n","        text = text[:max_len-2]\n","        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n","        #print(input_sequence)\n","        pad_len = max_len - len(input_sequence)\n","        \n","        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n","        tokens += [0] * pad_len\n","        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n","        segment_ids = [0] * max_len\n","        \n","        all_tokens.append(tokens)\n","        all_masks.append(pad_masks)\n","        all_segments.append(segment_ids)\n","    \n","    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"qRv_XbMZv34X","executionInfo":{"status":"ok","timestamp":1636766364266,"user_tz":-540,"elapsed":383,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"source":["# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n","\n","def build_model(bert_layer, max_len=512):\n","    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n","    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n","    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n","\n","    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n","    # (batch_size, 160, 1024)\n","    clf_output = sequence_output[:, 0, :] \n","    # [CLS]의 output 위치에서 재난 tweet 여부를 판단하는 output 출력\n","    \n","    if Dropout_num == 0:\n","        # Without Dropout\n","        out = Dense(1, activation='sigmoid')(clf_output)\n","    else:\n","        # With Dropout(Dropout_num), Dropout_num > 0\n","        x = Dropout(Dropout_num)(clf_output)\n","        out = Dense(1, activation='sigmoid')(x)\n","\n","    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n","    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n","    \n","    return model"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-MmbAoFxwWbK"},"source":["## <b> Pre-processing </b>"]},{"cell_type":"code","metadata":{"id":"fH_2KZf0wc3s","executionInfo":{"status":"ok","timestamp":1636765521812,"user_tz":-540,"elapsed":6,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"source":["# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n","def clean_tweets(tweet):\n","    \"\"\"Removes links and non-ASCII characters\"\"\"\n","    \n","    tweet = ''.join([x for x in tweet if x in string.printable])\n","    \n","    # Removing URLs\n","    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n","    \n","    return tweet"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Qilqu6lweVy","executionInfo":{"status":"ok","timestamp":1636765521812,"user_tz":-540,"elapsed":5,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"source":["# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n","def remove_emoji(text):\n","    emoji_pattern = re.compile(\"[\"\n","                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           u\"\\U00002702-\\U000027B0\"\n","                           u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    return emoji_pattern.sub(r'', text)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"GaYYipokwfrH","executionInfo":{"status":"ok","timestamp":1636765521813,"user_tz":-540,"elapsed":6,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"source":["# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n","def remove_punctuations(text):\n","    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n","    \n","    for p in punctuations:\n","        text = text.replace(p, f' {p} ')\n","\n","    text = text.replace('...', ' ... ')\n","    \n","    if '...' not in text:\n","        text = text.replace('..', ' ... ')\n","    \n","    return text"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"5zYvmsVLwif1","executionInfo":{"status":"ok","timestamp":1636765522400,"user_tz":-540,"elapsed":592,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"source":["# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n","abbreviations = {\n","    \"$\" : \" dollar \",\n","    \"€\" : \" euro \",\n","    \"4ao\" : \"for adults only\",\n","    \"a.m\" : \"before midday\",\n","    \"a3\" : \"anytime anywhere anyplace\",\n","    \"aamof\" : \"as a matter of fact\",\n","    \"acct\" : \"account\",\n","    \"adih\" : \"another day in hell\",\n","    \"afaic\" : \"as far as i am concerned\",\n","    \"afaict\" : \"as far as i can tell\",\n","    \"afaik\" : \"as far as i know\",\n","    \"afair\" : \"as far as i remember\",\n","    \"afk\" : \"away from keyboard\",\n","    \"app\" : \"application\",\n","    \"approx\" : \"approximately\",\n","    \"apps\" : \"applications\",\n","    \"asap\" : \"as soon as possible\",\n","    \"asl\" : \"age, sex, location\",\n","    \"atk\" : \"at the keyboard\",\n","    \"ave.\" : \"avenue\",\n","    \"aymm\" : \"are you my mother\",\n","    \"ayor\" : \"at your own risk\", \n","    \"b&b\" : \"bed and breakfast\",\n","    \"b+b\" : \"bed and breakfast\",\n","    \"b.c\" : \"before christ\",\n","    \"b2b\" : \"business to business\",\n","    \"b2c\" : \"business to customer\",\n","    \"b4\" : \"before\",\n","    \"b4n\" : \"bye for now\",\n","    \"b@u\" : \"back at you\",\n","    \"bae\" : \"before anyone else\",\n","    \"bak\" : \"back at keyboard\",\n","    \"bbbg\" : \"bye bye be good\",\n","    \"bbc\" : \"british broadcasting corporation\",\n","    \"bbias\" : \"be back in a second\",\n","    \"bbl\" : \"be back later\",\n","    \"bbs\" : \"be back soon\",\n","    \"be4\" : \"before\",\n","    \"bfn\" : \"bye for now\",\n","    \"blvd\" : \"boulevard\",\n","    \"bout\" : \"about\",\n","    \"brb\" : \"be right back\",\n","    \"bros\" : \"brothers\",\n","    \"brt\" : \"be right there\",\n","    \"bsaaw\" : \"big smile and a wink\",\n","    \"btw\" : \"by the way\",\n","    \"bwl\" : \"bursting with laughter\",\n","    \"c/o\" : \"care of\",\n","    \"cet\" : \"central european time\",\n","    \"cf\" : \"compare\",\n","    \"cia\" : \"central intelligence agency\",\n","    \"csl\" : \"can not stop laughing\",\n","    \"cu\" : \"see you\",\n","    \"cul8r\" : \"see you later\",\n","    \"cv\" : \"curriculum vitae\",\n","    \"cwot\" : \"complete waste of time\",\n","    \"cya\" : \"see you\",\n","    \"cyt\" : \"see you tomorrow\",\n","    \"dae\" : \"does anyone else\",\n","    \"dbmib\" : \"do not bother me i am busy\",\n","    \"diy\" : \"do it yourself\",\n","    \"dm\" : \"direct message\",\n","    \"dwh\" : \"during work hours\",\n","    \"e123\" : \"easy as one two three\",\n","    \"eet\" : \"eastern european time\",\n","    \"eg\" : \"example\",\n","    \"embm\" : \"early morning business meeting\",\n","    \"encl\" : \"enclosed\",\n","    \"encl.\" : \"enclosed\",\n","    \"etc\" : \"and so on\",\n","    \"faq\" : \"frequently asked questions\",\n","    \"fawc\" : \"for anyone who cares\",\n","    \"fb\" : \"facebook\",\n","    \"fc\" : \"fingers crossed\",\n","    \"fig\" : \"figure\",\n","    \"fimh\" : \"forever in my heart\", \n","    \"ft.\" : \"feet\",\n","    \"ft\" : \"featuring\",\n","    \"ftl\" : \"for the loss\",\n","    \"ftw\" : \"for the win\",\n","    \"fwiw\" : \"for what it is worth\",\n","    \"fyi\" : \"for your information\",\n","    \"g9\" : \"genius\",\n","    \"gahoy\" : \"get a hold of yourself\",\n","    \"gal\" : \"get a life\",\n","    \"gcse\" : \"general certificate of secondary education\",\n","    \"gfn\" : \"gone for now\",\n","    \"gg\" : \"good game\",\n","    \"gl\" : \"good luck\",\n","    \"glhf\" : \"good luck have fun\",\n","    \"gmt\" : \"greenwich mean time\",\n","    \"gmta\" : \"great minds think alike\",\n","    \"gn\" : \"good night\",\n","    \"g.o.a.t\" : \"greatest of all time\",\n","    \"goat\" : \"greatest of all time\",\n","    \"goi\" : \"get over it\",\n","    \"gps\" : \"global positioning system\",\n","    \"gr8\" : \"great\",\n","    \"gratz\" : \"congratulations\",\n","    \"gyal\" : \"girl\",\n","    \"h&c\" : \"hot and cold\",\n","    \"hp\" : \"horsepower\",\n","    \"hr\" : \"hour\",\n","    \"hrh\" : \"his royal highness\",\n","    \"ht\" : \"height\",\n","    \"ibrb\" : \"i will be right back\",\n","    \"ic\" : \"i see\",\n","    \"icq\" : \"i seek you\",\n","    \"icymi\" : \"in case you missed it\",\n","    \"idc\" : \"i do not care\",\n","    \"idgadf\" : \"i do not give a damn fuck\",\n","    \"idgaf\" : \"i do not give a fuck\",\n","    \"idk\" : \"i do not know\",\n","    \"ie\" : \"that is\",\n","    \"i.e\" : \"that is\",\n","    \"ifyp\" : \"i feel your pain\",\n","    \"IG\" : \"instagram\",\n","    \"iirc\" : \"if i remember correctly\",\n","    \"ilu\" : \"i love you\",\n","    \"ily\" : \"i love you\",\n","    \"imho\" : \"in my humble opinion\",\n","    \"imo\" : \"in my opinion\",\n","    \"imu\" : \"i miss you\",\n","    \"iow\" : \"in other words\",\n","    \"irl\" : \"in real life\",\n","    \"j4f\" : \"just for fun\",\n","    \"jic\" : \"just in case\",\n","    \"jk\" : \"just kidding\",\n","    \"jsyk\" : \"just so you know\",\n","    \"l8r\" : \"later\",\n","    \"lb\" : \"pound\",\n","    \"lbs\" : \"pounds\",\n","    \"ldr\" : \"long distance relationship\",\n","    \"lmao\" : \"laugh my ass off\",\n","    \"lmfao\" : \"laugh my fucking ass off\",\n","    \"lol\" : \"laughing out loud\",\n","    \"ltd\" : \"limited\",\n","    \"ltns\" : \"long time no see\",\n","    \"m8\" : \"mate\",\n","    \"mf\" : \"motherfucker\",\n","    \"mfs\" : \"motherfuckers\",\n","    \"mfw\" : \"my face when\",\n","    \"mofo\" : \"motherfucker\",\n","    \"mph\" : \"miles per hour\",\n","    \"mr\" : \"mister\",\n","    \"mrw\" : \"my reaction when\",\n","    \"ms\" : \"miss\",\n","    \"mte\" : \"my thoughts exactly\",\n","    \"nagi\" : \"not a good idea\",\n","    \"nbc\" : \"national broadcasting company\",\n","    \"nbd\" : \"not big deal\",\n","    \"nfs\" : \"not for sale\",\n","    \"ngl\" : \"not going to lie\",\n","    \"nhs\" : \"national health service\",\n","    \"nrn\" : \"no reply necessary\",\n","    \"nsfl\" : \"not safe for life\",\n","    \"nsfw\" : \"not safe for work\",\n","    \"nth\" : \"nice to have\",\n","    \"nvr\" : \"never\",\n","    \"nyc\" : \"new york city\",\n","    \"oc\" : \"original content\",\n","    \"og\" : \"original\",\n","    \"ohp\" : \"overhead projector\",\n","    \"oic\" : \"oh i see\",\n","    \"omdb\" : \"over my dead body\",\n","    \"omg\" : \"oh my god\",\n","    \"omw\" : \"on my way\",\n","    \"p.a\" : \"per annum\",\n","    \"p.m\" : \"after midday\",\n","    \"pm\" : \"prime minister\",\n","    \"poc\" : \"people of color\",\n","    \"pov\" : \"point of view\",\n","    \"pp\" : \"pages\",\n","    \"ppl\" : \"people\",\n","    \"prw\" : \"parents are watching\",\n","    \"ps\" : \"postscript\",\n","    \"pt\" : \"point\",\n","    \"ptb\" : \"please text back\",\n","    \"pto\" : \"please turn over\",\n","    \"qpsa\" : \"what happens\", #\"que pasa\",\n","    \"ratchet\" : \"rude\",\n","    \"rbtl\" : \"read between the lines\",\n","    \"rlrt\" : \"real life retweet\", \n","    \"rofl\" : \"rolling on the floor laughing\",\n","    \"roflol\" : \"rolling on the floor laughing out loud\",\n","    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n","    \"rt\" : \"retweet\",\n","    \"ruok\" : \"are you ok\",\n","    \"sfw\" : \"safe for work\",\n","    \"sk8\" : \"skate\",\n","    \"smh\" : \"shake my head\",\n","    \"sq\" : \"square\",\n","    \"srsly\" : \"seriously\", \n","    \"ssdd\" : \"same stuff different day\",\n","    \"tbh\" : \"to be honest\",\n","    \"tbs\" : \"tablespooful\",\n","    \"tbsp\" : \"tablespooful\",\n","    \"tfw\" : \"that feeling when\",\n","    \"thks\" : \"thank you\",\n","    \"tho\" : \"though\",\n","    \"thx\" : \"thank you\",\n","    \"tia\" : \"thanks in advance\",\n","    \"til\" : \"today i learned\",\n","    \"tl;dr\" : \"too long i did not read\",\n","    \"tldr\" : \"too long i did not read\",\n","    \"tmb\" : \"tweet me back\",\n","    \"tntl\" : \"trying not to laugh\",\n","    \"ttyl\" : \"talk to you later\",\n","    \"u\" : \"you\",\n","    \"u2\" : \"you too\",\n","    \"u4e\" : \"yours for ever\",\n","    \"utc\" : \"coordinated universal time\",\n","    \"w/\" : \"with\",\n","    \"w/o\" : \"without\",\n","    \"w8\" : \"wait\",\n","    \"wassup\" : \"what is up\",\n","    \"wb\" : \"welcome back\",\n","    \"wtf\" : \"what the fuck\",\n","    \"wtg\" : \"way to go\",\n","    \"wtpa\" : \"where the party at\",\n","    \"wuf\" : \"where are you from\",\n","    \"wuzup\" : \"what is up\",\n","    \"wywh\" : \"wish you were here\",\n","    \"yd\" : \"yard\",\n","    \"ygtr\" : \"you got that right\",\n","    \"ynk\" : \"you never know\",\n","    \"zzz\" : \"sleeping bored and tired\"\n","}"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"yRwzyX41wkUe","executionInfo":{"status":"ok","timestamp":1636765522401,"user_tz":-540,"elapsed":9,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"source":["# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n","def convert_abbrev(word):\n","    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"80LGqM4pwlik","executionInfo":{"status":"ok","timestamp":1636765522401,"user_tz":-540,"elapsed":9,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"source":["# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n","def convert_abbrev_in_text(text):\n","    tokens = word_tokenize(text)\n","    tokens = [convert_abbrev(word) for word in tokens]\n","    text = ' '.join(tokens)\n","    return text"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"6gwC7kOuzLrd","executionInfo":{"status":"ok","timestamp":1636765522401,"user_tz":-540,"elapsed":10,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"source":["# Thanks to https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data - \n","# author of this kernel read tweets in training data and figure out that some of them have errors:\n","if target_corrected:\n","    ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n","    train.loc[train['id'].isin(ids_with_target_error),'target'] = 0\n","    train[train['id'].isin(ids_with_target_error)]"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"V7MY89eAzNEw","executionInfo":{"status":"ok","timestamp":1636765522402,"user_tz":-540,"elapsed":10,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"source":["# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\n","if target_big_corrected:\n","    train[\"text\"] = train[\"text\"].apply(lambda x: clean_tweets(x))\n","    test[\"text\"] = test[\"text\"].apply(lambda x: clean_tweets(x))\n","    \n","    train[\"text\"] = train[\"text\"].apply(lambda x: remove_emoji(x))\n","    test[\"text\"] = test[\"text\"].apply(lambda x: remove_emoji(x))\n","    \n","    train[\"text\"] = train[\"text\"].apply(lambda x: remove_punctuations(x))\n","    test[\"text\"] = test[\"text\"].apply(lambda x: remove_punctuations(x))\n","    \n","    train[\"text\"] = train[\"text\"].apply(lambda x: convert_abbrev_in_text(x))\n","    test[\"text\"] = test[\"text\"].apply(lambda x: convert_abbrev_in_text(x))"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ow69YWsvWOb"},"source":["## <b> Build and train BERT model"]},{"cell_type":"code","metadata":{"id":"uqz8gaPivVLx","executionInfo":{"status":"ok","timestamp":1636766370548,"user_tz":-540,"elapsed":569,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"source":["# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n","# Load tokenizer from the bert layer\n","vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n","do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n","tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"6oNnxiVMvk5z","executionInfo":{"status":"ok","timestamp":1636766379370,"user_tz":-540,"elapsed":6061,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}}},"source":["# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n","# Encode the text into tokens, masks, and segment flags\n","train_input = bert_encode(train.text.values, tokenizer, max_len=160)\n","test_input = bert_encode(test.text.values, tokenizer, max_len=160)\n","train_labels = train.target.values"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dawAXCLRvmSD","executionInfo":{"status":"ok","timestamp":1636766381923,"user_tz":-540,"elapsed":927,"user":{"displayName":"TaeJun Park","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09014311516102133204"}},"outputId":"626dac87-5636-4db9-c340-aae797b2492f"},"source":["# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n","# Build BERT model with my tuning\n","model_BERT = build_model(bert_layer, max_len=160)\n","model_BERT.summary()"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["(None, 160, 1024)\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_word_ids (InputLayer)    [(None, 160)]        0           []                               \n","                                                                                                  \n"," input_mask (InputLayer)        [(None, 160)]        0           []                               \n","                                                                                                  \n"," segment_ids (InputLayer)       [(None, 160)]        0           []                               \n","                                                                                                  \n"," keras_layer (KerasLayer)       [(None, 1024),       335141889   ['input_word_ids[0][0]',         \n","                                 (None, 160, 1024)]               'input_mask[0][0]',             \n","                                                                  'segment_ids[0][0]']            \n","                                                                                                  \n"," tf.__operators__.getitem_1 (Sl  (None, 1024)        0           ['keras_layer[1][1]']            \n"," icingOpLambda)                                                                                   \n","                                                                                                  \n"," dense_1 (Dense)                (None, 1)            1025        ['tf.__operators__.getitem_1[0][0\n","                                                                 ]']                              \n","                                                                                                  \n","==================================================================================================\n","Total params: 335,142,914\n","Trainable params: 335,142,913\n","Non-trainable params: 1\n","__________________________________________________________________________________________________\n"]}]}]}